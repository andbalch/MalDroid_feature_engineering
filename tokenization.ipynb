{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenization\n",
    "The goal of this notebook is to decide on a pre-built tokenization algorithm for the sample JSON data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import pandas as pd \n",
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "# import tensorflow_text as text\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from numba import jit\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "## Test WordPiece Vocabulary\n",
    "WordPiece is BERTs tokenizer and we can use it to generate a custom vocabulary from our data. This is a test with a sample JSON to see what this kind of vocab would look like. This is a potential alternative to a custom algorithm as it creates a vocab iteratively from subwords. \n",
    "Requires: tensorflow_text_nightly and tf-nightly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Testing with different vocabulary length yielded mixed results where it was sometimes able to effectively pick out pertinent tokens but mostly fixated on smaller tokens found in the structure of the JSON file or behavior parameters, most of which being unnecessary. A custom algorithm or reference dictionary will likely need to be developed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes rand sample of data to form test dataset for dict building\n",
    "\n",
    "adware_sample = np.random.randint(0, high=886, size=(1,1))\n",
    "banking_sample = np.random.randint(0, high=2048, size=(1,1))\n",
    "riskware_sample = np.random.randint(0, high=2373, size=(1,1))\n",
    "sms_sample = np.random.randint(0, high=3852, size=(1,1))\n",
    "\n",
    "sample_mat = np.concatenate((adware_sample, banking_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, riskware_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, sms_sample), axis=0)\n",
    "\n",
    "dir_list = ['adware', 'banking', 'riskware', 'sms']\n",
    "\n",
    "# train_data = tf.data.TextLineDataset(str('adware\\\\' + os.listdir('adware')[sample_mat[0,0]] + '\\\\sample_for_analysis.apk.json'))\n",
    "\n",
    "# mat_index = 0\n",
    "# for sample_dir in dir_list: \n",
    "#     sample_list = os.listdir(sample_dir)\n",
    "#     if sample_dir is 'adware':\n",
    "#         start_index = 1\n",
    "#     else:\n",
    "#         start_index = 0\n",
    "#     for rand_ind in sample_mat[mat_index,start_index:]:\n",
    "#         train_data.concatenate(tf.data.TextLineDataset(str(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json')))\n",
    "#     mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer_params=dict(lower_case=True)\n",
    "# reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "# bert_vocab_args = dict(\n",
    "#     # The target vocabulary size\n",
    "#     vocab_size = 800,\n",
    "#     # Reserved tokens that must be included in the vocabulary\n",
    "#     reserved_tokens=reserved_tokens,\n",
    "#     # Arguments for `text.BertTokenizer`\n",
    "#     bert_tokenizer_params=bert_tokenizer_params,\n",
    "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "#     learn_params={},\n",
    "# )\n",
    "\n",
    "# test_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     train_data.shuffle(45).batch(45).prefetch(2),\n",
    "#     **bert_vocab_args\n",
    "# )\n",
    "\n",
    "# with open('test_vocab_rand_sample.txt', 'w') as f:\n",
    "#     for token in test_vocab:\n",
    "#         print(token, file=f)"
   ]
  },
  {
   "source": [
    "## Custom Tokenization\n",
    "Due to the apparent underperformance of pre-built models in deriving their own dictionaries, a custom method is required. The goal of this process is to derive reusable tokens and preserve the contextual, nested structure of the JSON file. We will do this by iteratively extracting tokens from a random sample until no meaningful (frequent) tokens remain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs rand sample from mat above\n",
    "sample_data = []\n",
    "\n",
    "mat_index = 0\n",
    "for sample_dir in dir_list: \n",
    "    sample_list = os.listdir(sample_dir)\n",
    "    for rand_ind in sample_mat[mat_index, 0:]:\n",
    "        with open(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json') as sample_file:\n",
    "            sample_data.append(sample_file.read().replace(\"\\n\", \" \"))\n",
    "    mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the frequency of all high level keys\n",
    "high_keys = []\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key in behavior.keys():\n",
    "            high_keys.append(key)\n",
    "\n",
    "high_uniq_keys, high_frequency = np.unique(high_keys, return_counts=True)\n",
    "print(high_uniq_keys)\n",
    "print(high_frequency)"
   ]
  },
  {
   "source": [
    "### High Level Tokens\n",
    "Inspection of high-level keys in a random sample revealed 11 keys\n",
    "* class\n",
    "* classType\n",
    "* interface\n",
    "* interfaceGroup\n",
    "* method\n",
    "* operationFlags\n",
    "* procname\n",
    "* subclass\n",
    "* tid\n",
    "* low\n",
    "* arguments\n",
    "#\n",
    "Of these 11, low contains lower-level features as a list of dicts. arguments contains a list of strings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the frequency of low-level tokens\n",
    "low_keys = []\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for low_behavior in behavior['low']:\n",
    "            for key in low_behavior.keys():\n",
    "                low_keys.append(key)\n",
    "\n",
    "low_uniq_keys, low_frequency = np.unique(low_keys, return_counts=True)\n",
    "print(low_uniq_keys)\n",
    "print(low_frequency)"
   ]
  },
  {
   "source": [
    "### Low Level Tokens\n",
    "Inspection of high-level tokens in a random sample revealed 12 keys\n",
    "* blob\n",
    "* id\n",
    "* methodName\n",
    "* method_name\n",
    "* parameters\n",
    "* read fd\n",
    "* socket fd\n",
    "* sysname\n",
    "* ts \n",
    "* type\n",
    "* write fd\n",
    "* xref\n",
    "#\n",
    "blob and parameters contain lowest-level features, where blob is a dict as a string and contains formatting abnormalities, it can also contain a normal string. parameters is a list of strings and/or ints\n",
    "# \n",
    "methodName and method_name differ where the former seems to be used in the context of a BINDER class. The latter seems exclusive to the ACCESS PERSONAL INFO class, but still with type binder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the frequency of low-level tokens\n",
    "# lowest_keys = []\n",
    "# for sample in sample_data:\n",
    "#     sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "#     for behavior in sample_json:\n",
    "#         for low_behavior in behavior['low']:\n",
    "#             for key in low_behavior.keys():\n",
    "#                 if key == 'blob':\n",
    "#                     string_dict = low_behavior['blob']\n",
    "#                     if type(string_dict) is dict:\n",
    "#                         for lowest_key in string_dict.keys():\n",
    "#                             lowest_keys.append(lowest_key)\n",
    "#                     else:\n",
    "#                         string_dict = string_dict.replace(\"L,\", \",\")\n",
    "#                         string_dict = string_dict.replace(\"L}\", \"}\")\n",
    "#                         string_dict = string_dict.replace(\"u\\'\", \"\\'\")\n",
    "#                         string_dict = string_dict.replace(\"\\'\", \"\\\"\")\n",
    "#                         try:\n",
    "#                             string_dict = ast.literal_eval(string_dict)\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         if type(string_dict) is dict:\n",
    "#                             for lowest_key in string_dict.keys():\n",
    "#                                 lowest_keys.append(lowest_key)\n",
    "\n",
    "# lowest_uniq_keys, lowest_frequency = np.unique(lowest_keys, return_counts=True)\n",
    "# print(lowest_uniq_keys)\n",
    "# print(lowest_frequency)"
   ]
  },
  {
   "source": [
    "### Lowest Level Tokens\n",
    "Keys in blob contained wildly varying frequencies, see the sample output below:\n",
    "#\n",
    "['devicename' 'dirname' 'filename' 'flags' 'host' 'mode' 'pid' 'port'\n",
    " 'query_data' 'returnValue' 'size' 'socket domain' 'socket protocol'\n",
    " 'socket type' 'status' 'tid' 'type']\n",
    " #\n",
    "[215011   1163 650182  58849   3282  60002   8869   3282    123   3282\n",
    " 530656   5927   5927   5927   2277   8869   5506]\n",
    " #\n",
    "I'm not entirely sure how much this data will be explicitly tokenized by a custom algorithm. There are many many inconsistencies in the parsing of the string into JSON that causes it to throw out a lot of values. This could likely be compensated for, but in the interest of time it may be better to avoid it and explore the useage of a pre-built algo on the string, like what is planned for parameters and arguments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Token Extraction\n",
    "With high and low level keys extracted, the next step of building the tokens is to look at the number and distribution of possible values for each token. This will be done by making a dataframe for each unique key with the key being the col and each observed value as the rows. The dataframe will then be made into a piechart with matplotlib and saved to jpg for inspection with a caption containing the number of unique values.\n",
    "#\n",
    "Excluded keys:\n",
    "* parameters\n",
    "* id\n",
    "* blob\n",
    "* xref\n",
    "* ts \n",
    "* arguments\n",
    "* tid\n",
    "#\n",
    "These will be excluded from tokenization as they are identifiers and/or contain lists or dicts not for explicit tokenization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "exclude_keys = ['parameters', 'id', 'blob', 'xref', 'ts', 'arguments', 'tid']\n",
    "\n",
    "master_dict = {\n",
    "    'methodName': [],\n",
    "    'method_name': [],\n",
    "    'read fd': [],\n",
    "    'socket fd': [],\n",
    "    'sysname': [],\n",
    "    'type': [],\n",
    "    'write fd': [],\n",
    "    'class': [],\n",
    "    'classType': [],\n",
    "    'interface': [],\n",
    "    'interfaceGroup': [],\n",
    "    'method': [],\n",
    "    'operationFlags': [],\n",
    "    'procname': [],\n",
    "    'subclass': []\n",
    "}\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key, value in behavior.items():\n",
    "            if key not in exclude_keys and key != 'low':\n",
    "                master_dict[key].append(value)\n",
    "            elif key == 'low':\n",
    "                for low_behavior in value:\n",
    "                    for low_key, low_value in low_behavior.items():\n",
    "                        if low_key not in exclude_keys:\n",
    "                            master_dict[low_key].append(low_value)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                continue"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref_key, total_values in master_dict.items():\n",
    "    if ref_key in high_uniq_keys:\n",
    "        total_count = high_frequency[np.where(high_uniq_keys == ref_key)[0][0]]\n",
    "        num_values = len(total_values)\n",
    "        len_match = total_count == num_values\n",
    "        print(ref_key + ': ')\n",
    "        print(len_match)\n",
    "        print('\\n')\n",
    "    elif ref_key in low_uniq_keys:\n",
    "        total_count = low_frequency[np.where(low_uniq_keys == ref_key)[0][0]]\n",
    "        num_values = len(total_values)\n",
    "        len_match = total_count == num_values\n",
    "        print(ref_key + ': ')\n",
    "        print(len_match)\n",
    "        print('\\n')\n",
    "    df = pd.DataFrame(total_values, columns=[ref_key])\n",
    "    with open('token_extraction\\\\csvs\\\\' + ref_key + '.csv', 'w') as write_csv:\n",
    "        df.to_csv(write_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(dataframe, filename):\n",
    "    dataframe.fillna(\"NaN\", inplace=True)\n",
    "    dataframe = dataframe.astype(\"category\")\n",
    "\n",
    "    key_name = dataframe.columns.values.tolist()[0]\n",
    "    dataframe[key_name] = dataframe[key_name].cat.as_ordered()\n",
    "    values = dataframe[key_name].to_numpy()\n",
    "    categories, counts = np.unique(values, return_counts=True)\n",
    "\n",
    "    plt.style.use('seaborn-muted')\n",
    "    sns.color_palette(\"muted\")\n",
    "\n",
    "    plot, (top, bottom) = plt.subplots(2, figsize=[10, 20])\n",
    "    plot.suptitle(\"Makeup and Frequency of Unique Values for Key: \" + key_name)\n",
    "\n",
    "    patches, text = top.pie(counts, labels=categories)\n",
    "    top.axis('equal')\n",
    "    top.set_title(\"Makeup of Unique Values for Key: \" + key_name)\n",
    "    percentage = 100.*counts/counts.sum()\n",
    "    labels = ['{0} - {1:1.2f}% ({2})'.format(i,j,k) for i,j,k in zip(categories, percentage, counts)]\n",
    "    top.legend(patches, labels, frameon=False, loc = 'center left', bbox_to_anchor=(-0.5, 0.5))\n",
    "\n",
    "    sns.histplot(ax=bottom, data=dataframe, y=key_name, hue=key_name, shrink=.8, legend=False, discrete=True)\n",
    "    bottom.set_title(\"Frequency of Unique Values for Key: \" + key_name)\n",
    "\n",
    "    plot.savefig('token_extraction\\\\visualizations\\\\' + filename + '.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_keys = []\n",
    "\n",
    "# WARNING: The csv files 'class', 'type' and 'sysname' are too large to be included in the repo. These will have to be created locally to fully utilize this script.\n",
    "\n",
    "# for csv_file in os.listdir('token_extraction\\\\csvs'):\n",
    "#     if csv_file.replace('.csv', '') not in omit_keys:\n",
    "#         with open('token_extraction\\\\csvs\\\\' + csv_file) as csv_path:\n",
    "#             data = pd.read_csv(csv_path)\n",
    "#         make_plots(data, csv_file.replace('.csv', ''))"
   ]
  },
  {
   "source": [
    "### Results\n",
    "Most plots turned out reasonably well, but ones with more unique values were problematic. These problematic keys were:\n",
    "* method\n",
    "* method_name\n",
    "* procname\n",
    "#\n",
    "To accomodate these we will implement a script to log percentages and counts for each unique value into a text file for each key."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_keys = []\n",
    "\n",
    "# for key in alt_keys:\n",
    "#     with open('token_extraction\\\\csvs\\\\' + key + '.csv') as open_csv:\n",
    "#         key_data = pd.read_csv(open_csv)\n",
    "#     key_data.fillna(\"NaN\", inplace=True)\n",
    "#     values = key_data[key].to_numpy()\n",
    "#     categories, counts = np.unique(values, return_counts=True)\n",
    "#     categories = categories[np.argsort(-counts)]\n",
    "#     counts = counts[np.argsort(-counts)]\n",
    "#     percentage = 100.*counts/counts.sum()\n",
    "#     lines = ['{0} - {1:1.2f}% ({2}) \\n'.format(i,j,k) for i,j,k in zip(categories, percentage, counts)]\n",
    "#     with open('token_extraction\\\\raw\\\\' + key + '.txt', 'w') as write_txt:\n",
    "#         write_txt.writelines(lines)"
   ]
  },
  {
   "source": [
    "### Making the Dictionary\n",
    "With the data collected, we are now ready to prepare the finalized reference dictionary for a future tokenization function. Due to the nature of our raw data, pre-built tokenizers cannot be used with our custom dict as they rely on traditional separations between words and sentences which our data does not have. Thus, the actual tokenizer function will likely be built using Python's RegEx library so our dictionary structure needs to be optimized for compatability. \n",
    "#### Functions of the Dictionary\n",
    "* Compatable with n-grams (multiple delimiters at the same index)\n",
    "* Cheaply iterable (RegEx will split the behaviors into lists of tokens that then need to be referenced with indicies, this can't take very long)\n",
    "#### Dictionary Format\n",
    "* Int of token index as key\n",
    "* Value will be touple of RegEx expression and list of corresponding unique tokens as plain strings\n",
    "### Important Notes\n",
    "* interfaceGroup does not offer any additional information over interface, therefore it is not accounted for in the vocabulary and should be excluded from the final data\n",
    "* methodName will not be included in vocabulary or the final dataset. There are thousands of unique values in a sample size of only 400 and these values really only amount to a combination of the interface, method, and arguments keys. Therefore, the model should be able to form generalizations from the first two and arguments can be tokenized and implemented to provide the same level of data if it is necessary.\n",
    "* The vocabulary may turn out to be around 1000 tokens. Due to the extreme length of each sample, this could become very expensive during training, so it is worth considering taking a look at the hidden vectors of each token after a few epochs and then concatenating those in a key that are very similar, forming n grams and decreasing training cost. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code looks at the makeup of unique procname values in hopes of creating tokens from the frequency of different subvalues\n",
    "\n",
    "first_level = []\n",
    "second_level = []\n",
    "third_level = []\n",
    "procname_info = open(\"token_extraction\\\\raw\\\\procname.txt\", 'r')\n",
    "for line in procname_info: \n",
    "    value = line.split(\" - \")[0]\n",
    "    levels = value.split('.')\n",
    "    level_count = 0\n",
    "    for level in levels:\n",
    "        if level_count == 0:\n",
    "            first_level.append(level)\n",
    "        elif level_count == 1: \n",
    "            second_level.append(level)\n",
    "        elif level_count == 2:\n",
    "            third_level.append('.'.join(levels[2:]))\n",
    "        level_count += 1\n",
    "\n",
    "first_uniq, first_counts = np.unique(first_level, return_counts=True)\n",
    "second_uniq, second_counts = np.unique(second_level, return_counts=True)\n",
    "third_uniq, third_counts = np.unique(third_level, return_counts=True)\n",
    "\n",
    "print(first_uniq)\n",
    "print(first_counts)\n",
    "print('\\n')\n",
    "\n",
    "print(second_uniq)\n",
    "print(second_counts)\n",
    "print('\\n')\n",
    "\n",
    "print(third_uniq)\n",
    "print(third_counts)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formats and creates n grams for procname tokens\n",
    "method_info = open(\"token_extraction\\\\raw\\\\procname.txt\", 'r')\n",
    "formatted_lines = []\n",
    "count = 683\n",
    "thread_flag = False\n",
    "mmreader_flag = False\n",
    "pool_flag = False\n",
    "for line in method_info:\n",
    "    value = line.split(\" - \")[0]\n",
    "    re_value = re.escape(value)\n",
    "    if 'Thread' in value:\n",
    "        if not thread_flag:\n",
    "            value = 'Thread-<anything>'\n",
    "            re_value = 'Thread-(?:..|...)'\n",
    "            thread_flag = True\n",
    "        else:\n",
    "            continue\n",
    "    elif 'mmreader' in value:\n",
    "        if not mmreader_flag:\n",
    "            value = 'com.android.mmreader<anything>'\n",
    "            re_value = 'com\\.android\\.mmreader(?:...|....)'\n",
    "            mmreader_flag = True\n",
    "        else:\n",
    "            continue\n",
    "    elif 'pool-' in value:\n",
    "        if not pool_flag:\n",
    "            value = 'pool-<anything>-thread-<anything>'\n",
    "            re_value = 'pool-.-thread-.'\n",
    "            pool_flag = True\n",
    "        else:\n",
    "            continue\n",
    "    formatted_lines.append('{1}: (\\'\\\"procname\\\": \\\"{0}\\\"\\', [\\'\\\"procname\\\": \\\"{2}\\\"\\']),\\n'.format(re_value, count, value))\n",
    "    count += 1\n",
    "method_info.close()\n",
    "\n",
    "with open('formatted_procname.txt', 'w') as write:\n",
    "    for line in formatted_lines:\n",
    "        write.write(line)"
   ]
  },
  {
   "source": [
    "## Light vocab testing\n",
    "Just to make sure the vocab is working correctly we want to do a quick test where we look at a few samples, tokenize them, and then look at which tokens are not converted to indicices. \n",
    "### Process\n",
    "1. Define the regex expression from vocabulary.txt dict\n",
    "2. Tokenize with regex without a lookbehind assertion\n",
    "3. Make file of leftover tokens and analyze\n",
    "### Results\n",
    "* Fairly good, seemingly no tokens that should be in the vocabulary were returned as oov.\n",
    "* Most entries were made up of ts, parameter, blob, etc keys\n",
    "* A lot of commas, brackets, and braces were extracted where they shouldn't have or left in\n",
    "- Need to reasses how these are extracted so meaningless ones are not tokenized and meaningful ones are"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt') as vocab_path:\n",
    "    vocab_file = vocab_path.read()\n",
    "\n",
    "delimiter_list = []\n",
    "vocab = ast.literal_eval(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for packed_value in vocab.values():\n",
    "    delimiter, literal = packed_value\n",
    "    delimiter_list.append(str(delimiter))\n",
    "\n",
    "regex_pattern = re.compile('|'.join(delimiter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftover_tokens = []\n",
    "\n",
    "@jit(parallel=True, fastmath=True)\n",
    "def take_tokens(sample):\n",
    "    return re.split(regex_pattern, sample)\n",
    "\n",
    "for sample in sample_data:\n",
    "    leftover_tokens.append(take_tokens(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_oov_tokens.txt', 'w') as write_tokens:\n",
    "    for sample_tokens in leftover_tokens:\n",
    "        for token in sample_tokens:\n",
    "            write_tokens.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
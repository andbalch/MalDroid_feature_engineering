{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenization\n",
    "The goal of this notebook is to decide on a pre-built tokenization algorithm for the sample JSON data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "source": [
    "## Test WordPiece Vocabulary\n",
    "WordPiece is BERTs tokenizer and we can use it to generate a custom vocabulary from our data. This is a test with a sample JSON to see what this kind of vocab would look like. This is a potential alternative to a custom algorithm as it creates a vocab iteratively from subwords. \n",
    "Requires: tensorflow_text_nightly and tf-nightly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Testing with different vocabulary length yielded mixed results where it was sometimes able to effectively pick out pertinent tokens but mostly fixated on smaller tokens found in the structure of the JSON file or behavior parameters, most of which being unnecessary. A custom algorithm or reference dictionary will likely need to be developed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes rand sample of data to form test dataset for dict building\n",
    "\n",
    "# adware_sample = np.random.randint(0, high=887, size=(1,15))\n",
    "# banking_sample = np.random.randint(0, high=2049, size=(1,15))\n",
    "# riskware_sample = np.random.randint(0, high=2374, size=(1,15))\n",
    "# sms_sample = np.random.randint(0, high=3853, size=(1,15))\n",
    "\n",
    "# sample_mat = np.concatenate((adware_sample, banking_sample), axis=0)\n",
    "# sample_mat = np.concatenate((sample_mat, riskware_sample), axis=0)\n",
    "# sample_mat = np.concatenate((sample_mat, sms_sample), axis=0)\n",
    "\n",
    "# dir_list = ['adware', 'banking', 'riskware', 'sms']\n",
    "\n",
    "# train_data = tf.data.TextLineDataset(str('adware\\\\' + os.listdir('adware')[sample_mat[0,0]] + '\\\\sample_for_analysis.apk.json'))\n",
    "\n",
    "# for sample_dir in dir_list: \n",
    "#     mat_index = 0\n",
    "#     sample_list = os.listdir(sample_dir)\n",
    "#     for rand_ind in sample_mat[mat_index,0:]:\n",
    "#         train_data.concatenate(tf.data.TextLineDataset(str(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 800,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "# test_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     train_data.shuffle(45).batch(45).prefetch(2),\n",
    "#     **bert_vocab_args\n",
    "# )\n",
    "\n",
    "# with open('test_vocab_rand_sample.txt', 'w') as f:\n",
    "#     for token in test_vocab:\n",
    "#         print(token, file=f)"
   ]
  },
  {
   "source": [
    "## Custom Tokenization\n",
    "Due to the apparent underperformance of pre-built models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
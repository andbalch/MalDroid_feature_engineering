{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenization\n",
    "The goal of this notebook is to decide on a pre-built tokenization algorithm for the sample JSON data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "import tensorflow_text as text\n",
    "import ast"
   ]
  },
  {
   "source": [
    "## Test WordPiece Vocabulary\n",
    "WordPiece is BERTs tokenizer and we can use it to generate a custom vocabulary from our data. This is a test with a sample JSON to see what this kind of vocab would look like. This is a potential alternative to a custom algorithm as it creates a vocab iteratively from subwords. \n",
    "Requires: tensorflow_text_nightly and tf-nightly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Testing with different vocabulary length yielded mixed results where it was sometimes able to effectively pick out pertinent tokens but mostly fixated on smaller tokens found in the structure of the JSON file or behavior parameters, most of which being unnecessary. A custom algorithm or reference dictionary will likely need to be developed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes rand sample of data to form test dataset for dict building\n",
    "\n",
    "adware_sample = np.random.randint(0, high=886, size=(1,100))\n",
    "banking_sample = np.random.randint(0, high=2048, size=(1,100))\n",
    "riskware_sample = np.random.randint(0, high=2373, size=(1,100))\n",
    "sms_sample = np.random.randint(0, high=3852, size=(1,100))\n",
    "\n",
    "sample_mat = np.concatenate((adware_sample, banking_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, riskware_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, sms_sample), axis=0)\n",
    "\n",
    "dir_list = ['adware', 'banking', 'riskware', 'sms']\n",
    "\n",
    "# train_data = tf.data.TextLineDataset(str('adware\\\\' + os.listdir('adware')[sample_mat[0,0]] + '\\\\sample_for_analysis.apk.json'))\n",
    "\n",
    "# mat_index = 0\n",
    "# for sample_dir in dir_list: \n",
    "#     sample_list = os.listdir(sample_dir)\n",
    "#     if sample_dir is 'adware':\n",
    "#         start_index = 1\n",
    "#     else:\n",
    "#         start_index = 0\n",
    "#     for rand_ind in sample_mat[mat_index,start_index:]:\n",
    "#         train_data.concatenate(tf.data.TextLineDataset(str(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json')))\n",
    "#     mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer_params=dict(lower_case=True)\n",
    "# reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "# bert_vocab_args = dict(\n",
    "#     # The target vocabulary size\n",
    "#     vocab_size = 800,\n",
    "#     # Reserved tokens that must be included in the vocabulary\n",
    "#     reserved_tokens=reserved_tokens,\n",
    "#     # Arguments for `text.BertTokenizer`\n",
    "#     bert_tokenizer_params=bert_tokenizer_params,\n",
    "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "#     learn_params={},\n",
    "# )\n",
    "\n",
    "# test_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     train_data.shuffle(45).batch(45).prefetch(2),\n",
    "#     **bert_vocab_args\n",
    "# )\n",
    "\n",
    "# with open('test_vocab_rand_sample.txt', 'w') as f:\n",
    "#     for token in test_vocab:\n",
    "#         print(token, file=f)"
   ]
  },
  {
   "source": [
    "## Custom Tokenization\n",
    "Due to the apparent underperformance of pre-built models in deriving their own dictionaries, a custom method is required. The goal of this process is to derive reusable tokens and preserve the contextual, nested structure of the JSON file. We will do this by iteratively extracting tokens from a random sample until no meaningful (frequent) tokens remain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs rand sample from mat above\n",
    "sample_data = []\n",
    "\n",
    "mat_index = 0\n",
    "for sample_dir in dir_list: \n",
    "    sample_list = os.listdir(sample_dir)\n",
    "    for rand_ind in sample_mat[mat_index, 0:]:\n",
    "        with open(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json') as sample_file:\n",
    "            sample_data.append(sample_file.read().replace(\"\\n\", \" \"))\n",
    "    mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['arguments' 'class' 'classType' 'interface' 'interfaceGroup' 'low'\n 'method' 'operationFlags' 'procname' 'subclass' 'tid']\n[  22198 7169259   78358   22198   22198 7169259   22198   70940   81202\n    4094   81202]\n"
     ]
    }
   ],
   "source": [
    "# See the frequency of all high level keys\n",
    "high_keys = []\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key in behavior.keys():\n",
    "            high_keys.append(key)\n",
    "\n",
    "high_uniq_keys, high_frequency = np.unique(high_keys, return_counts=True)\n",
    "print(high_uniq_keys)\n",
    "print(high_frequency)"
   ]
  },
  {
   "source": [
    "### High Level Tokens\n",
    "Inspection of high-level keys in a random sample revealed 11 keys\n",
    "* class\n",
    "* classType\n",
    "* interface\n",
    "* interfaceGroup\n",
    "* method\n",
    "* operationFlags\n",
    "* procname\n",
    "* subclass\n",
    "* tid\n",
    "* low\n",
    "* arguments\n",
    "#\n",
    "Of these 11, low contains lower-level features as a list of dicts. arguments contains a list of strings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['blob' 'id' 'methodName' 'method_name' 'parameters' 'read fd' 'socket fd'\n 'sysname' 'ts' 'type' 'write fd' 'xref']\n[ 904587 7995480   22198    2844 7065859    1830    1250 7973282 7995480\n 7995480    1830  823766]\n"
     ]
    }
   ],
   "source": [
    "# See the frequency of low-level tokens\n",
    "low_keys = []\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for low_behavior in behavior['low']:\n",
    "            for key in low_behavior.keys():\n",
    "                low_keys.append(key)\n",
    "\n",
    "low_uniq_keys, low_frequency = np.unique(low_keys, return_counts=True)\n",
    "print(low_uniq_keys)\n",
    "print(low_frequency)"
   ]
  },
  {
   "source": [
    "### Low Level Tokens\n",
    "Inspection of high-level tokens in a random sample revealed 12 keys\n",
    "* blob\n",
    "* id\n",
    "* methodName\n",
    "* method_name\n",
    "* parameters\n",
    "* read fd\n",
    "* socket fd\n",
    "* sysname\n",
    "* ts \n",
    "* type\n",
    "* write fd\n",
    "* xref\n",
    "#\n",
    "blob and parameters contain lowest-level features, where blob is a dict as a string and contains formatting abnormalities, it can also contain a normal string. parameters is a list of strings and/or ints\n",
    "# \n",
    "methodName and method_name differ where the former seems to be used in the context of a BINDER class. The latter seems exclusive to the ACCESS PERSONAL INFO class, but still with type binder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7822db5fa6df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlowest_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msample_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'behaviors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dynamic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'host'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_json\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlow_behavior\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# See the frequency of low-level tokens\n",
    "# lowest_keys = []\n",
    "# for sample in sample_data:\n",
    "#     sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "#     for behavior in sample_json:\n",
    "#         for low_behavior in behavior['low']:\n",
    "#             for key in low_behavior.keys():\n",
    "#                 if key == 'blob':\n",
    "#                     string_dict = low_behavior['blob']\n",
    "#                     if type(string_dict) is dict:\n",
    "#                         for lowest_key in string_dict.keys():\n",
    "#                             lowest_keys.append(lowest_key)\n",
    "#                     else:\n",
    "#                         string_dict = string_dict.replace(\"L,\", \",\")\n",
    "#                         string_dict = string_dict.replace(\"L}\", \"}\")\n",
    "#                         string_dict = string_dict.replace(\"u\\'\", \"\\'\")\n",
    "#                         string_dict = string_dict.replace(\"\\'\", \"\\\"\")\n",
    "#                         try:\n",
    "#                             string_dict = ast.literal_eval(string_dict)\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         if type(string_dict) is dict:\n",
    "#                             for lowest_key in string_dict.keys():\n",
    "#                                 lowest_keys.append(lowest_key)\n",
    "\n",
    "# lowest_uniq_keys, lowest_frequency = np.unique(lowest_keys, return_counts=True)\n",
    "# print(lowest_uniq_keys)\n",
    "# print(lowest_frequency)"
   ]
  },
  {
   "source": [
    "### Lowest Level Tokens\n",
    "Keys in blob contained wildly varying frequencies, see the sample output below:\n",
    "#\n",
    "['devicename' 'dirname' 'filename' 'flags' 'host' 'mode' 'pid' 'port'\n",
    " 'query_data' 'returnValue' 'size' 'socket domain' 'socket protocol'\n",
    " 'socket type' 'status' 'tid' 'type']\n",
    " #\n",
    "[215011   1163 650182  58849   3282  60002   8869   3282    123   3282\n",
    " 530656   5927   5927   5927   2277   8869   5506]\n",
    " #\n",
    "I'm not entirely sure how much this data will be explicitly tokenized by a custom algorithm. There are many many inconsistencies in the parsing of the string into JSON that causes it to throw out a lot of values. This could likely be compensated for, but in the interest of time it may be better to avoid it and explore the useage of a pre-built algo on the string, like what is planned for parameters and arguments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Token Extraction\n",
    "With high and low level keys extracted, the next step of building the tokens is to look at the number and distribution of possible values for each token. This will be done by making a dataframe for each unique key with the key being the col and each observed value as the rows. The dataframe will then be made into a piechart with matplotlib and saved to jpg for inspection with a caption containing the number of unique values.\n",
    "#\n",
    "Excluded keys:\n",
    "* parameters\n",
    "* id\n",
    "* blob\n",
    "* xref\n",
    "* ts \n",
    "* arguments\n",
    "* tid\n",
    "#\n",
    "These will be excluded from tokenization as they are identifiers and/or contain lists or dicts not for explicit tokenization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "exclude_keys = ['parameters', 'id', 'blob', 'xref', 'ts', 'arguments', 'tid']\n",
    "\n",
    "master_dict = {\n",
    "    'methodName': [],\n",
    "    'method_name': [],\n",
    "    'read fd': [],\n",
    "    'socket fd': [],\n",
    "    'sysname': [],\n",
    "    'type': [],\n",
    "    'write fd': [],\n",
    "    'class': [],\n",
    "    'classType': [],\n",
    "    'interface': [],\n",
    "    'interfaceGroup': [],\n",
    "    'method': [],\n",
    "    'operationFlags': [],\n",
    "    'procname': [],\n",
    "    'subclass': []\n",
    "}\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key, value in behavior.items():\n",
    "            if key not in exclude_keys:\n",
    "                master_dict[key].append(value)\n",
    "            else:\n",
    "                continue\n",
    "            for low_behavior in behavior['low']:\n",
    "                for low_key, low_value in low_behavior.items():\n",
    "                    if low_key not in exclude_keys:\n",
    "                        master_dict[low_key].append(low_value)\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "for ref_key, total_values in master_dict.items():\n",
    "    df = pd.DataFrame(total_values, columns=[ref_key])\n",
    "    with open('token_extraction\\\\csvs\\\\' + ref_key + '.csv', 'w') as write_csv:\n",
    "        df.to_csv(write_csv, index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## ID and TS Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
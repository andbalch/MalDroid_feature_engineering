{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenization\n",
    "The goal of this notebook is to decide on a pre-built tokenization algorithm for the sample JSON data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "source": [
    "## Test WordPiece Vocabulary\n",
    "WordPiece is BERTs tokenizer and we can use it to generate a custom vocabulary from our data. This is a test with a sample JSON to see what this kind of vocab would look like. This is a potential alternative to a custom algorithm as it creates a vocab iteratively from subwords. \n",
    "Requires: tensorflow_text_nightly and tf-nightly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Testing with different vocabulary length yielded mixed results where it was sometimes able to effectively pick out pertinent tokens but mostly fixated on smaller tokens found in the structure of the JSON file or behavior parameters, most of which being unnecessary. A custom algorithm or reference dictionary will likely need to be developed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes rand sample of data to form test dataset for dict building\n",
    "\n",
    "adware_sample = np.random.randint(0, high=887, size=(1,50))\n",
    "banking_sample = np.random.randint(0, high=2049, size=(1,50))\n",
    "riskware_sample = np.random.randint(0, high=2374, size=(1,50))\n",
    "sms_sample = np.random.randint(0, high=3853, size=(1,50))\n",
    "\n",
    "sample_mat = np.concatenate((adware_sample, banking_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, riskware_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, sms_sample), axis=0)\n",
    "\n",
    "dir_list = ['adware', 'banking', 'riskware', 'sms']\n",
    "\n",
    "# train_data = tf.data.TextLineDataset(str('adware\\\\' + os.listdir('adware')[sample_mat[0,0]] + '\\\\sample_for_analysis.apk.json'))\n",
    "\n",
    "# mat_index = 0\n",
    "# for sample_dir in dir_list: \n",
    "#     sample_list = os.listdir(sample_dir)\n",
    "#     if sample_dir is 'adware':\n",
    "#         start_index = 1\n",
    "#     else:\n",
    "#         start_index = 0\n",
    "#     for rand_ind in sample_mat[mat_index,start_index:]:\n",
    "#         train_data.concatenate(tf.data.TextLineDataset(str(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json')))\n",
    "#     mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer_params=dict(lower_case=True)\n",
    "# reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "# bert_vocab_args = dict(\n",
    "#     # The target vocabulary size\n",
    "#     vocab_size = 800,\n",
    "#     # Reserved tokens that must be included in the vocabulary\n",
    "#     reserved_tokens=reserved_tokens,\n",
    "#     # Arguments for `text.BertTokenizer`\n",
    "#     bert_tokenizer_params=bert_tokenizer_params,\n",
    "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "#     learn_params={},\n",
    "# )\n",
    "\n",
    "# test_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     train_data.shuffle(45).batch(45).prefetch(2),\n",
    "#     **bert_vocab_args\n",
    "# )\n",
    "\n",
    "# with open('test_vocab_rand_sample.txt', 'w') as f:\n",
    "#     for token in test_vocab:\n",
    "#         print(token, file=f)"
   ]
  },
  {
   "source": [
    "## Custom Tokenization\n",
    "Due to the apparent underperformance of pre-built models in deriving their own dictionaries, a custom method is required. The goal of this process is to derive reusable tokens and preserve the contextual, nested structure of the JSON file. We will do this by iteratively extracting tokens from a random sample until no meaningful (frequent) tokens remain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs rand sample from mat above\n",
    "sample_data = []\n",
    "\n",
    "mat_index = 0\n",
    "for sample_dir in dir_list: \n",
    "    sample_list = os.listdir(sample_dir)\n",
    "    for rand_ind in sample_mat[mat_index,0:]:\n",
    "        with open(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json') as sample_file:\n",
    "            sample_data.append(sample_file.read().replace(\"\\n\", \" \"))\n",
    "    mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['arguments' 'class' 'classType' 'interface' 'interfaceGroup' 'low'\n 'method' 'operationFlags' 'procname' 'subclass' 'tid']\n[  21036 4805170   64309   21036   21036 4805170   21036   56733   71113\n   10006   71113]\n"
     ]
    }
   ],
   "source": [
    "# See the frequency of all high level keys\n",
    "high_keys = []\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key in behavior.keys():\n",
    "            high_keys.append(key)\n",
    "\n",
    "uniq_keys, key_frequency = np.unique(high_keys, return_counts=True)\n",
    "print(uniq_keys)\n",
    "print(key_frequency)"
   ]
  },
  {
   "source": [
    "### High Level Tokens\n",
    "Inspection of high-level keys in a random sample revealed 11 keys\n",
    "* class\n",
    "* classType\n",
    "* interface\n",
    "* interfaceGroup\n",
    "* method\n",
    "* operationFlags\n",
    "* procname\n",
    "* subclass\n",
    "* tid\n",
    "* low\n",
    "* arguments\n",
    "#\n",
    "Of these 11, low contains lower-level features. tid and arguments will likely not be used as tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['blob' 'id' 'methodName' 'method_name' 'parameters' 'read fd' 'socket fd'\n 'sysname' 'ts' 'type' 'write fd' 'xref']\n[ 685381 5426236   21036    6804 4713021    2109    1102 5405200 5426236\n 5426236    2109  618406]\n"
     ]
    }
   ],
   "source": [
    "# See the frequency of low-level tokens\n",
    "low_keys = []\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for low_behavior in behavior['low']:\n",
    "            for key in low_behavior.keys():\n",
    "                low_keys.append(key)\n",
    "\n",
    "uniq_keys, key_frequency = np.unique(low_keys, return_counts=True)\n",
    "print(uniq_keys)\n",
    "print(key_frequency)"
   ]
  },
  {
   "source": [
    "# uniq, counts = np.unique(value_list, return_counts=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "source": [
    "## ID and TS Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenization\n",
    "The goal of this notebook is to decide on a pre-built tokenization algorithm for the sample JSON data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import os\n",
    "import json \n",
    "import numpy as np\n",
    "import tensorflow_text as text\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "## Test WordPiece Vocabulary\n",
    "WordPiece is BERTs tokenizer and we can use it to generate a custom vocabulary from our data. This is a test with a sample JSON to see what this kind of vocab would look like. This is a potential alternative to a custom algorithm as it creates a vocab iteratively from subwords. \n",
    "Requires: tensorflow_text_nightly and tf-nightly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Testing with different vocabulary length yielded mixed results where it was sometimes able to effectively pick out pertinent tokens but mostly fixated on smaller tokens found in the structure of the JSON file or behavior parameters, most of which being unnecessary. A custom algorithm or reference dictionary will likely need to be developed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes rand sample of data to form test dataset for dict building\n",
    "\n",
    "adware_sample = np.random.randint(0, high=886, size=(1,100))\n",
    "banking_sample = np.random.randint(0, high=2048, size=(1,100))\n",
    "riskware_sample = np.random.randint(0, high=2373, size=(1,100))\n",
    "sms_sample = np.random.randint(0, high=3852, size=(1,100))\n",
    "\n",
    "sample_mat = np.concatenate((adware_sample, banking_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, riskware_sample), axis=0)\n",
    "sample_mat = np.concatenate((sample_mat, sms_sample), axis=0)\n",
    "\n",
    "dir_list = ['adware', 'banking', 'riskware', 'sms']\n",
    "\n",
    "# train_data = tf.data.TextLineDataset(str('adware\\\\' + os.listdir('adware')[sample_mat[0,0]] + '\\\\sample_for_analysis.apk.json'))\n",
    "\n",
    "# mat_index = 0\n",
    "# for sample_dir in dir_list: \n",
    "#     sample_list = os.listdir(sample_dir)\n",
    "#     if sample_dir is 'adware':\n",
    "#         start_index = 1\n",
    "#     else:\n",
    "#         start_index = 0\n",
    "#     for rand_ind in sample_mat[mat_index,start_index:]:\n",
    "#         train_data.concatenate(tf.data.TextLineDataset(str(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json')))\n",
    "#     mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer_params=dict(lower_case=True)\n",
    "# reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "# bert_vocab_args = dict(\n",
    "#     # The target vocabulary size\n",
    "#     vocab_size = 800,\n",
    "#     # Reserved tokens that must be included in the vocabulary\n",
    "#     reserved_tokens=reserved_tokens,\n",
    "#     # Arguments for `text.BertTokenizer`\n",
    "#     bert_tokenizer_params=bert_tokenizer_params,\n",
    "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "#     learn_params={},\n",
    "# )\n",
    "\n",
    "# test_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     train_data.shuffle(45).batch(45).prefetch(2),\n",
    "#     **bert_vocab_args\n",
    "# )\n",
    "\n",
    "# with open('test_vocab_rand_sample.txt', 'w') as f:\n",
    "#     for token in test_vocab:\n",
    "#         print(token, file=f)"
   ]
  },
  {
   "source": [
    "## Custom Tokenization\n",
    "Due to the apparent underperformance of pre-built models in deriving their own dictionaries, a custom method is required. The goal of this process is to derive reusable tokens and preserve the contextual, nested structure of the JSON file. We will do this by iteratively extracting tokens from a random sample until no meaningful (frequent) tokens remain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs rand sample from mat above\n",
    "sample_data = []\n",
    "\n",
    "mat_index = 0\n",
    "for sample_dir in dir_list: \n",
    "    sample_list = os.listdir(sample_dir)\n",
    "    for rand_ind in sample_mat[mat_index, 0:]:\n",
    "        with open(sample_dir + '\\\\' + sample_list[rand_ind] + '\\\\sample_for_analysis.apk.json') as sample_file:\n",
    "            sample_data.append(sample_file.read().replace(\"\\n\", \" \"))\n",
    "    mat_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the frequency of all high level keys\n",
    "high_keys = []\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key in behavior.keys():\n",
    "            high_keys.append(key)\n",
    "\n",
    "high_uniq_keys, high_frequency = np.unique(high_keys, return_counts=True)\n",
    "print(high_uniq_keys)\n",
    "print(high_frequency)"
   ]
  },
  {
   "source": [
    "### High Level Tokens\n",
    "Inspection of high-level keys in a random sample revealed 11 keys\n",
    "* class\n",
    "* classType\n",
    "* interface\n",
    "* interfaceGroup\n",
    "* method\n",
    "* operationFlags\n",
    "* procname\n",
    "* subclass\n",
    "* tid\n",
    "* low\n",
    "* arguments\n",
    "#\n",
    "Of these 11, low contains lower-level features as a list of dicts. arguments contains a list of strings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the frequency of low-level tokens\n",
    "low_keys = []\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for low_behavior in behavior['low']:\n",
    "            for key in low_behavior.keys():\n",
    "                low_keys.append(key)\n",
    "\n",
    "low_uniq_keys, low_frequency = np.unique(low_keys, return_counts=True)\n",
    "print(low_uniq_keys)\n",
    "print(low_frequency)"
   ]
  },
  {
   "source": [
    "### Low Level Tokens\n",
    "Inspection of high-level tokens in a random sample revealed 12 keys\n",
    "* blob\n",
    "* id\n",
    "* methodName\n",
    "* method_name\n",
    "* parameters\n",
    "* read fd\n",
    "* socket fd\n",
    "* sysname\n",
    "* ts \n",
    "* type\n",
    "* write fd\n",
    "* xref\n",
    "#\n",
    "blob and parameters contain lowest-level features, where blob is a dict as a string and contains formatting abnormalities, it can also contain a normal string. parameters is a list of strings and/or ints\n",
    "# \n",
    "methodName and method_name differ where the former seems to be used in the context of a BINDER class. The latter seems exclusive to the ACCESS PERSONAL INFO class, but still with type binder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the frequency of low-level tokens\n",
    "# lowest_keys = []\n",
    "# for sample in sample_data:\n",
    "#     sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "#     for behavior in sample_json:\n",
    "#         for low_behavior in behavior['low']:\n",
    "#             for key in low_behavior.keys():\n",
    "#                 if key == 'blob':\n",
    "#                     string_dict = low_behavior['blob']\n",
    "#                     if type(string_dict) is dict:\n",
    "#                         for lowest_key in string_dict.keys():\n",
    "#                             lowest_keys.append(lowest_key)\n",
    "#                     else:\n",
    "#                         string_dict = string_dict.replace(\"L,\", \",\")\n",
    "#                         string_dict = string_dict.replace(\"L}\", \"}\")\n",
    "#                         string_dict = string_dict.replace(\"u\\'\", \"\\'\")\n",
    "#                         string_dict = string_dict.replace(\"\\'\", \"\\\"\")\n",
    "#                         try:\n",
    "#                             string_dict = ast.literal_eval(string_dict)\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         if type(string_dict) is dict:\n",
    "#                             for lowest_key in string_dict.keys():\n",
    "#                                 lowest_keys.append(lowest_key)\n",
    "\n",
    "# lowest_uniq_keys, lowest_frequency = np.unique(lowest_keys, return_counts=True)\n",
    "# print(lowest_uniq_keys)\n",
    "# print(lowest_frequency)"
   ]
  },
  {
   "source": [
    "### Lowest Level Tokens\n",
    "Keys in blob contained wildly varying frequencies, see the sample output below:\n",
    "#\n",
    "['devicename' 'dirname' 'filename' 'flags' 'host' 'mode' 'pid' 'port'\n",
    " 'query_data' 'returnValue' 'size' 'socket domain' 'socket protocol'\n",
    " 'socket type' 'status' 'tid' 'type']\n",
    " #\n",
    "[215011   1163 650182  58849   3282  60002   8869   3282    123   3282\n",
    " 530656   5927   5927   5927   2277   8869   5506]\n",
    " #\n",
    "I'm not entirely sure how much this data will be explicitly tokenized by a custom algorithm. There are many many inconsistencies in the parsing of the string into JSON that causes it to throw out a lot of values. This could likely be compensated for, but in the interest of time it may be better to avoid it and explore the useage of a pre-built algo on the string, like what is planned for parameters and arguments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Token Extraction\n",
    "With high and low level keys extracted, the next step of building the tokens is to look at the number and distribution of possible values for each token. This will be done by making a dataframe for each unique key with the key being the col and each observed value as the rows. The dataframe will then be made into a piechart with matplotlib and saved to jpg for inspection with a caption containing the number of unique values.\n",
    "#\n",
    "Excluded keys:\n",
    "* parameters\n",
    "* id\n",
    "* blob\n",
    "* xref\n",
    "* ts \n",
    "* arguments\n",
    "* tid\n",
    "#\n",
    "These will be excluded from tokenization as they are identifiers and/or contain lists or dicts not for explicit tokenization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "exclude_keys = ['parameters', 'id', 'blob', 'xref', 'ts', 'arguments', 'tid']\n",
    "\n",
    "master_dict = {\n",
    "    'methodName': [],\n",
    "    'method_name': [],\n",
    "    'read fd': [],\n",
    "    'socket fd': [],\n",
    "    'sysname': [],\n",
    "    'type': [],\n",
    "    'write fd': [],\n",
    "    'class': [],\n",
    "    'classType': [],\n",
    "    'interface': [],\n",
    "    'interfaceGroup': [],\n",
    "    'method': [],\n",
    "    'operationFlags': [],\n",
    "    'procname': [],\n",
    "    'subclass': []\n",
    "}\n",
    "\n",
    "for sample in sample_data:\n",
    "    sample_json = json.loads(sample)['behaviors']['dynamic']['host']\n",
    "    for behavior in sample_json:\n",
    "        for key, value in behavior.items():\n",
    "            if key not in exclude_keys and key != 'low':\n",
    "                master_dict[key].append(value)\n",
    "            elif key == 'low':\n",
    "                for low_behavior in value:\n",
    "                    for low_key, low_value in low_behavior.items():\n",
    "                        if low_key not in exclude_keys:\n",
    "                            master_dict[low_key].append(low_value)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                continue"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref_key, total_values in master_dict.items():\n",
    "    if ref_key in high_uniq_keys:\n",
    "        total_count = high_frequency[np.where(high_uniq_keys == ref_key)[0][0]]\n",
    "        num_values = len(total_values)\n",
    "        len_match = total_count == num_values\n",
    "        print(ref_key + ': ')\n",
    "        print(len_match)\n",
    "        print('\\n')\n",
    "    elif ref_key in low_uniq_keys:\n",
    "        total_count = low_frequency[np.where(low_uniq_keys == ref_key)[0][0]]\n",
    "        num_values = len(total_values)\n",
    "        len_match = total_count == num_values\n",
    "        print(ref_key + ': ')\n",
    "        print(len_match)\n",
    "        print('\\n')\n",
    "    df = pd.DataFrame(total_values, columns=[ref_key])\n",
    "    with open('token_extraction\\\\csvs\\\\' + ref_key + '.csv', 'w') as write_csv:\n",
    "        df.to_csv(write_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(dataframe, filename):\n",
    "    dataframe.fillna(\"NaN\", inplace=True)\n",
    "    dataframe = dataframe.astype(\"category\")\n",
    "\n",
    "    key_name = dataframe.columns.values.tolist()[0]\n",
    "    dataframe[key_name] = dataframe[key_name].cat.as_ordered()\n",
    "    values = dataframe[key_name].to_numpy()\n",
    "    categories, counts = np.unique(values, return_counts=True)\n",
    "\n",
    "    plt.style.use('seaborn-muted')\n",
    "    sns.color_palette(\"muted\")\n",
    "\n",
    "    plot, (top, bottom) = plt.subplots(2, figsize=[10, 20])\n",
    "    plot.suptitle(\"Makeup and Frequency of Unique Values for Key: \" + key_name)\n",
    "\n",
    "    patches, text = top.pie(counts, labels=categories)\n",
    "    top.axis('equal')\n",
    "    top.set_title(\"Makeup of Unique Values for Key: \" + key_name)\n",
    "    percentage = 100.*counts/counts.sum()\n",
    "    labels = ['{0} - {1:1.2f}% ({2})'.format(i,j,k) for i,j,k in zip(categories, percentage, counts)]\n",
    "    top.legend(patches, labels, frameon=False, loc = 'center left', bbox_to_anchor=(-0.5, 0.5))\n",
    "\n",
    "    sns.histplot(ax=bottom, data=dataframe, y=key_name, hue=key_name, shrink=.8, legend=False, discrete=True)\n",
    "    bottom.set_title(\"Frequency of Unique Values for Key: \" + key_name)\n",
    "\n",
    "    plot.savefig('token_extraction\\\\visualizations\\\\' + filename + '.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_keys = []\n",
    "\n",
    "# WARNING: The csv files 'class', 'type' and 'sysname' are too large to be included in the repo. These will have to be created locally to fully utilize this script.\n",
    "\n",
    "# for csv_file in os.listdir('token_extraction\\\\csvs'):\n",
    "#     if csv_file.replace('.csv', '') not in omit_keys:\n",
    "#         with open('token_extraction\\\\csvs\\\\' + csv_file) as csv_path:\n",
    "#             data = pd.read_csv(csv_path)\n",
    "#         make_plots(data, csv_file.replace('.csv', ''))"
   ]
  },
  {
   "source": [
    "### Results\n",
    "Most plots turned out reasonably well, but ones with more unique values were problematic. These problematic keys were:\n",
    "* method\n",
    "* method_name\n",
    "* procname\n",
    "#\n",
    "To accomodate these we will implement a script to log percentages and counts for each unique value into a text file for each key."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_keys = []\n",
    "\n",
    "# for key in alt_keys:\n",
    "#     with open('token_extraction\\\\csvs\\\\' + key + '.csv') as open_csv:\n",
    "#         key_data = pd.read_csv(open_csv)\n",
    "#     key_data.fillna(\"NaN\", inplace=True)\n",
    "#     values = key_data[key].to_numpy()\n",
    "#     categories, counts = np.unique(values, return_counts=True)\n",
    "#     categories = categories[np.argsort(-counts)]\n",
    "#     counts = counts[np.argsort(-counts)]\n",
    "#     percentage = 100.*counts/counts.sum()\n",
    "#     lines = ['{0} - {1:1.2f}% ({2}) \\n'.format(i,j,k) for i,j,k in zip(categories, percentage, counts)]\n",
    "#     with open('token_extraction\\\\raw\\\\' + key + '.txt', 'w') as write_txt:\n",
    "#         write_txt.writelines(lines)"
   ]
  },
  {
   "source": [
    "## ID and TS Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
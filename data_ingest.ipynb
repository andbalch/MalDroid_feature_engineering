{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('MalDroid_feature_engineering': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6df8f70e17f22c8259d4d2a30cfb8ccb378ae2ffe8034be92b64d9d6528b27db"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MalDroid Data Ingest\n",
    "This notebook handles the retrieval of MalDroid malware sample analysis (sample_for_analysis.apk.json) files from the MalDroid repo (http://205.174.165.80/CICDataset/MalDroid-2020/Dataset/Capturing_logs/). \n",
    "## Process\n",
    "1. Take the URIs of the .tar.gz files for each sample from MalDroid_ref_raw.csv, extract them to .tar and then to a directory *malware class*/*hash*\n",
    "2. Check to ensure the sample_for_analysis.apk.json file does not throw an error when opening. If yes, directory will be deleted and script will move to next sample\n",
    "3. Move the sample_for_analysis.apk.json file from the *malware class*/*hash*/sample_for_analysis.apk subdirectory to under *malware class*/*hash*/ and delete the subdirectory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tarfile\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MalDroid_ref_raw.csv') as ref_raw_path:\n",
    "    ref_raw = pd.read_csv(ref_raw_path)\n",
    "ref_raw = ref_raw[['hash', 'URI', 'malware_class']]\n",
    "ref_raw = ref_raw.drop([13076])\n",
    "# Drops final row; does not contain a sample due to parsing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_hashes = []\n",
    "large_hashes = []\n",
    "last_hash = 'start'\n",
    "\n",
    "for sample in ref_raw.itertuples(index=False):\n",
    "    last_hash = sample.hash\n",
    "    if '.tar.gz' in sample.URI:\n",
    "        base_path = sample.malware_class.lower() + '/' + sample.hash\n",
    "        temp_file = urllib.request.urlretrieve(sample.URI, filename=None)[0]\n",
    "        file = tarfile.open(temp_file)\n",
    "        file.extractall(base_path)\n",
    "        file.close()\n",
    "        with open(base_path + '/sample_for_analysis.apk/sample_for_analysis.apk.json') as default_path:\n",
    "            try:\n",
    "                json.load(default_path)\n",
    "            except:\n",
    "                issue_hashes.append(sample.hash)\n",
    "                continue\n",
    "        json_size = os.stat(base_path + '/sample_for_analysis.apk/sample_for_analysis.apk.json').st_size\n",
    "        if json_size >= 100000000:\n",
    "            # GitHub will not host files larger than 100 MB, this removes and logs these files for compatability\n",
    "            large_hashes.append(sample.hash)\n",
    "            shutil.rmtree(base_path)\n",
    "            continue\n",
    "        shutil.move(base_path + '/sample_for_analysis.apk/sample_for_analysis.apk.json', base_path + '/sample_for_analysis.apk.json')\n",
    "        shutil.rmtree(base_path + '/sample_for_analysis.apk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_df = pd.DataFrame(issue_hashes, columns = ['hash'])\n",
    "with open('issue_hashes.csv', 'w') as issue_write:\n",
    "    issue_df.to_csv(issue_write)\n",
    "\n",
    "large_df = pd.DataFrame(large_hashes, columns = ['hash'])\n",
    "with open('large_hashes.csv', 'w') as large_write:\n",
    "    large_df.to_csv(large_write)\n",
    "\n",
    "with open('last_hash_processed.txt', 'w') as resume_write:\n",
    "    resume_write.write(last_hash)"
   ]
  }
 ]
}